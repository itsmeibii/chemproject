{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Descriptors\n",
    "import warnings\n",
    "import shutup\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "from rdkit import RDLogger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# X,y = load_diabetes(return_X_y=True)\n",
    "# #print(load_diabetes().DESCR)\n",
    "\n",
    "# #Random Forest\n",
    "# fpl = Pipeline([\n",
    "#     (\"scale\", StandardScaler()),\n",
    "#     (\"model\", RandomForestRegressor())\n",
    "# ])\n",
    "# # fmodel = fpl.fit(X,y)\n",
    "# fpred = fmodel.predict(X)\n",
    "# plt.scatter(fpred,y, color = 'green')\n",
    "\n",
    "\n",
    "#KNeighbours\n",
    "# knpl = Pipeline([\n",
    "#     (\"scale\", StandardScaler()),\n",
    "#     (\"model\", KNeighborsRegressor())\n",
    "# ])\n",
    "# model = knpl.fit(X,y)\n",
    "# prediction = model.predict(X)\n",
    "# plt.scatter(prediction,y, color = 'red')\n",
    "\n",
    "#fpred is a lot better, more linear which means it is more consistent with the original targets\n",
    "#The models above were used to predict the same data that they were trained upon\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#Now, I will attempt to train both models on half the data set, then predict the other half, and then plot those predictions against the targets, much like what I think cross validation is\n",
    "\n",
    "#Random Forest\n",
    "# truefmodel = fpl.fit(X[:221],y[:221])\n",
    "# truefpred = truefmodel.predict(X[-221:])\n",
    "# plt.scatter(truefpred,y[-221:],color = 'green')\n",
    "\n",
    "# #KNeighbours\n",
    "# truekmodel = knpl.fit(X[:221],y[:221])\n",
    "# truekpred = truekmodel.predict(X[-221:])\n",
    "# plt.scatter(truekpred,y[-221:], color = 'red')\n",
    "\n",
    "#Have just now become aware of a train_test_split function that wouldve done this for me\n",
    "\n",
    "\n",
    "#Now we see that they really arent all that much different without hyperparameter tuning. Random Forest was just a lot better at predicting when it is predicting the data that it was trained on\n",
    "#This was a sort of half baked two fold cross validation that was done manually\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "#Now I will perform cross validation how it is intended\n",
    "\n",
    "#fpl.get_params()\n",
    "# gridmodel = GridSearchCV(estimator = fpl,\n",
    "#                          param_grid = {\n",
    "#                              'model__n_estimators':[25,50,75,100,125,150,200], #Num of trees, decision makers\n",
    "#                              'model__max_depth':[1,2,3,4,5,6,7,8,9,10] #Num of decisions made by each tree\n",
    "                              \n",
    "#                          },\n",
    "#                          scoring = {'precision':make_scorer(precision_score), 'recall':make_scorer(recall_score)},\n",
    "#                          refit = 'precision',\n",
    "#                          cv = 3,\n",
    "#                          n_jobs = -1)\n",
    "\n",
    "# gridmodel.fit(X,y)\n",
    "# data = pd.DataFrame(gridmodel.cv_results_) \n",
    "#print(data.loc[data['mean_test_score'].idxmax()])\n",
    "\n",
    "#For Outliers\n",
    "# outl = IsolationForest().fit(X)\n",
    "# print(Counter(outl.predict(X)))\n",
    "#70 Outliers, a large chunk of data\n",
    "\n",
    "\n",
    "#The above takes forever to run\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "#I do want to become familiar with train test split, so will play around with that\n",
    "#Random Split\n",
    "#X_train, X_temp, Y_train, Y_temp = train_test_split(X,y,test_size = 0.2,random_state = 42, test_size = 0.2 #20% of the data will be used for testing\n",
    "                                                    #)\n",
    "#Further split the testing set into validation and actual testing, with validation being for fine tuning\n",
    "#How do i do that\n",
    "#COME BACK TO THIS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "with open('./database.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(data).transpose()\n",
    "\n",
    "\n",
    "def bemis_murcko_scaffold(smiles, retasmol):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    if retasmol:\n",
    "        return scaffold\n",
    "    else :\n",
    "        return Chem.MolToSmiles(scaffold)\n",
    "df['Scaffold'] = df['smiles'].apply(bemis_murcko_scaffold, args=(True,))\n",
    "dfm = df[[ 'iupac', 'smiles','Scaffold', 'calc_h']].copy()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#print(df.head())\n",
    "\n",
    "#Prints columns as nums but types are objects, so fixing that\n",
    "# for column in df.columns:\n",
    "#     if df[column].dtype == 'object':\n",
    "#         df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "#<FEATURIZATION /> -------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def getMolDescriptors(mol, missingVal=None):\n",
    "    RDLogger.DisableLog('rdApp.*') #OH MY GOD THIS WORKSSSSSSS\n",
    "    res = {}\n",
    "    output = io.StringIO()\n",
    "    with redirect_stdout(output):\n",
    "            for nm,fn in Descriptors.descList:\n",
    "                if hasattr(fn, '__deprecated__') and fn.__deprecated__:\n",
    "                    print(f\"Skipping deprecated descriptor function: {nm}\")\n",
    "                    continue\n",
    "                    # some of the descriptor fucntions can throw errors if they fail, catch those here:\n",
    "                try:\n",
    "                        val = fn(mol)\n",
    "                except:\n",
    "                        # print the error message:\n",
    "                    \n",
    "                        # and set the descriptor value to whatever missingVal is\n",
    "                    val = missingVal\n",
    "                res[nm] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iupac', 'smiles', 'Scaffold', 'calc_h', 'mol', 'MaxAbsEStateIndex',\n",
      "       'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed',\n",
      "       ...\n",
      "       'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene',\n",
      "       'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene',\n",
      "       'fr_unbrch_alkane', 'fr_urea'],\n",
      "      dtype='object', length=425)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "dfm['mol'] = dfm['smiles'].apply(Chem.MolFromSmiles)\n",
    "allDescs = dfm['mol'].apply(getMolDescriptors)\n",
    "descdf = pd.DataFrame(allDescs.tolist(), index=dfm.index)\n",
    "\n",
    "# Merge descdf with dfm\n",
    "dfm = pd.concat([dfm, descdf], axis=1)\n",
    "\n",
    "# Now dfm should contain all descriptors as columns\n",
    "print(dfm.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Bemis-Murcko scaffold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df.head())\n",
    "# image = Draw.MolToImage(df.at[df.index[3],'Scaffold'])\n",
    "# image.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#No duplicate scaffolds\n",
    "unique = df['Scaffold'].unique()\n",
    "\n",
    "#---------------------\n",
    "# X = df.drop(columns=['calc_h'])\n",
    "# Y = df['calc_h']\n",
    "#---------------------\n",
    "\n",
    "\n",
    "#Seperate training and testing scaffolds\n",
    "train_scaff, test_scaff = train_test_split(unique,test_size= 0.2, random_state=42)\n",
    "#Convert scaffolds back into molecules\n",
    "train_data = dfm[dfm['Scaffold'].isin(train_scaff)]\n",
    "test_data = dfm[dfm['Scaffold'].isin(test_scaff)]\n",
    "#Pipeline wont work with objects and the scaffold was only needed for splitting\n",
    "#<FEATURIZATION> ----------\n",
    "smilescol = dfm['smiles']\n",
    "X_train = train_data.drop(columns=['calc_h'])\n",
    "y_train = train_data['calc_h']\n",
    "\n",
    "X_test = test_data.drop(columns=['calc_h'])\n",
    "y_test = test_data['calc_h']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     (\"scale\", StandardScaler()),\n",
    "#     (\"feature_selection\", SelectKBest(score_func=f_regression, k = 100)),\n",
    "#     (\"model\", RandomForestRegressor())\n",
    "# ])\n",
    "# model = pipeline.fit(X_train,y_train)\n",
    "# prediction = model.predict(X_train)\n",
    "# plt.scatter(prediction,y_train, color = 'red')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
